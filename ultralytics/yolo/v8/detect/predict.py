# Ultralytics YOLO ğŸš€, GPL-3.0 license

import hydra
import torch
import argparse
import time
from pathlib import Path
from collections import defaultdict, deque
import cv2
import torch
import torch.backends.cudnn as cudnn
from numpy import random
from ultralytics.yolo.engine.predictor import BasePredictor
from ultralytics.yolo.utils import DEFAULT_CONFIG, ROOT, ops
from ultralytics.yolo.utils.checks import check_imgsz
from ultralytics.yolo.utils.plotting import Annotator, colors, save_one_box

from deep_sort_pytorch.utils.parser import get_config
from deep_sort_pytorch.deep_sort import DeepSort
import time
import numpy as np

palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)
data_deque = {}

deepsort = None

if torch.cuda.is_available():
    print("GPU is available!")
    print("Device:", torch.cuda.get_device_name(0))
else:
    print("GPU is not available.")

def init_tracker():
    global deepsort
    cfg_deep = get_config()
    cfg_deep.merge_from_file("deep_sort_pytorch/configs/deep_sort.yaml")

    deepsort= DeepSort(cfg_deep.DEEPSORT.REID_CKPT,
                            max_dist=cfg_deep.DEEPSORT.MAX_DIST, min_confidence=cfg_deep.DEEPSORT.MIN_CONFIDENCE,
                            nms_max_overlap=cfg_deep.DEEPSORT.NMS_MAX_OVERLAP, max_iou_distance=cfg_deep.DEEPSORT.MAX_IOU_DISTANCE,
                            max_age=2,  # max_ageë¥¼ 2ë¡œ ì„¤ì •í•˜ì—¬ 2 í”„ë ˆì„ ì´í›„ì— ê°ì²´ê°€ ì‚¬ë¼ì¡Œë‹¤ê³  íŒë‹¨
                            n_init=cfg_deep.DEEPSORT.N_INIT, nn_budget=cfg_deep.DEEPSORT.NN_BUDGET,
                            use_cuda=True)
##########################################################################################
def xyxy_to_xywh(*xyxy):
    """" Calculates the relative bounding box from absolute pixel values. """
    bbox_left = min([xyxy[0].item(), xyxy[2].item()])
    bbox_top = min([xyxy[1].item(), xyxy[3].item()])
    bbox_w = abs(xyxy[0].item() - xyxy[2].item())
    bbox_h = abs(xyxy[1].item() - xyxy[3].item())
    x_c = (bbox_left + bbox_w / 2)
    y_c = (bbox_top + bbox_h / 2)
    w = bbox_w
    h = bbox_h
    return x_c, y_c, w, h

def xyxy_to_tlwh(bbox_xyxy):
    tlwh_bboxs = []
    for i, box in enumerate(bbox_xyxy):
        x1, y1, x2, y2 = [int(i) for i in box]
        top = x1
        left = y1
        w = int(x2 - x1)
        h = int(y2 - y1)
        tlwh_obj = [top, left, w, h]
        tlwh_bboxs.append(tlwh_obj)
    return tlwh_bboxs

def compute_color_for_labels(label):
    """
    Simple function that adds fixed color depending on the class
    """
    if label == 0: # person
        color = (85,45,255)
    else:
        color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]
    return tuple(color)

def draw_border(img, pt1, pt2, color, thickness, r, d):
    x1,y1 = pt1
    x2,y2 = pt2
    # Top left
    cv2.line(img, (x1 + r, y1), (x1 + r + d, y1), color, thickness)
    cv2.line(img, (x1, y1 + r), (x1, y1 + r + d), color, thickness)
    cv2.ellipse(img, (x1 + r, y1 + r), (r, r), 180, 0, 90, color, thickness)
    # Top right
    cv2.line(img, (x2 - r, y1), (x2 - r - d, y1), color, thickness)
    cv2.line(img, (x2, y1 + r), (x2, y1 + r + d), color, thickness)
    cv2.ellipse(img, (x2 - r, y1 + r), (r, r), 270, 0, 90, color, thickness)
    # Bottom left
    cv2.line(img, (x1 + r, y2), (x1 + r + d, y2), color, thickness)
    cv2.line(img, (x1, y2 - r), (x1, y2 - r - d), color, thickness)
    cv2.ellipse(img, (x1 + r, y2 - r), (r, r), 90, 0, 90, color, thickness)
    # Bottom right
    cv2.line(img, (x2 - r, y2), (x2 - r - d, y2), color, thickness)
    cv2.line(img, (x2, y2 - r), (x2, y2 - r - d), color, thickness)
    cv2.ellipse(img, (x2 - r, y2 - r), (r, r), 0, 0, 90, color, thickness)

    cv2.rectangle(img, (x1 + r, y1), (x2 - r, y2), color, -1, cv2.LINE_AA)
    cv2.rectangle(img, (x1, y1 + r), (x2, y2 - r - d), color, -1, cv2.LINE_AA)
    
    cv2.circle(img, (x1 +r, y1+r), 2, color, 12)
    cv2.circle(img, (x2 -r, y1+r), 2, color, 12)
    cv2.circle(img, (x1 +r, y2-r), 2, color, 12)
    cv2.circle(img, (x2 -r, y2-r), 2, color, 12)
    
    return img

def UI_box(x, img, color=None, label=None, line_thickness=None):
    # Plots one bounding box on image img
    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness
    color = color or [random.randint(0, 255) for _ in range(3)]
    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))
    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)
    if label:
        tf = max(tl - 1, 1)  # font thickness
        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]

        img = draw_border(img, (c1[0], c1[1] - t_size[1] -3), (c1[0] + t_size[0], c1[1]+3), color, 1, 8, 2)

        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)

def draw_boxes(img, bbox, names, object_id, identities=None, object_times=None, offset=(0, 0)):
    # Remove tracked points from buffer if object is lost
    for key in list(data_deque):
        if key not in identities:
            data_deque.pop(key)

    for i, box in enumerate(bbox):
        x1, y1, x2, y2 = [int(i) for i in box]
        x1 += offset[0]
        x2 += offset[0]
        y1 += offset[1]
        y2 += offset[1]

        # Center of bottom edge
        center = (int((x2+x1)/ 2), int((y2+y2)/2))

        # Object ID
        id = int(identities[i]) if identities is not None else 0

        # Create new buffer for new object
        if id not in data_deque:  
            data_deque[id] = deque(maxlen=64)
        color = compute_color_for_labels(object_id[i])
        obj_name = names[object_id[i]]

        # ì‹œê°„ ì •ë³´ ì¶”ê°€
        time_on_screen = object_times.get(id, 0)
        label = f'ID {id}: {obj_name} | Time: {time_on_screen:.2f}s'

        # Add center to buffer
        data_deque[id].appendleft(center)
        UI_box(box, img, label=label, color=color, line_thickness=2)

        # Draw trail
        for j in range(1, len(data_deque[id])):
            if data_deque[id][j - 1] is None or data_deque[id][j] is None:
                continue
            thickness = int(np.sqrt(64 / float(j + j)) * 1.5)
            cv2.line(img, data_deque[id][j - 1], data_deque[id][j], color, thickness)
    return img

class DetectionPredictor(BasePredictor):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.tracked_objects = defaultdict(lambda: {'frames_seen': 0, 'last_seen': -1})
        self.object_times = {}  # ê° ê°ì²´ì˜ í™”ë©´ì— ë‚˜íƒ€ë‚œ ì‹œê°„ì„ ì €ì¥
        self.saved_objects = set()  # ì´ë¯¸ ì €ì¥ëœ ê°ì²´ì˜ IDë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ì§‘í•©

        # ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.object_save_dir = Path("./saved_object")
        self.object_save_dir.mkdir(parents=True, exist_ok=True)

        # FPS ì„¤ì •
        if hasattr(self.dataset, 'fps') and self.dataset.fps:
            self.fps = self.dataset.fps
        else:
            self.fps = 30  # ê¸°ë³¸ê°’

        # ì²« ë²ˆì§¸ í”„ë ˆì„ì˜ í¬ê¸°ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì´ˆê¸°í™”
        self.frame_width = None
        self.frame_height = None
        self.video_writer = None  # VideoWriterëŠ” ì²« ë²ˆì§¸ í”„ë ˆì„ì—ì„œ ì´ˆê¸°í™”

    def get_annotator(self, img):
        return Annotator(img, line_width=self.args.line_thickness, example=str(self.model.names))

    def preprocess(self, img):
        # GPU ì‚¬ìš©ì„ ìœ„í•´ ëª¨ë¸ ì¥ì¹˜ë¥¼ ê°•ì œë¡œ CUDAë¡œ ì„¤ì •
        self.model.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        img = torch.from_numpy(img).to(self.model.device)  # GPUë¡œ ë°ì´í„° ì´ë™
        img = img.half() if self.model.fp16 else img.float()  # uint8 to fp16/32
        img /= 255  # 0 - 255 to 0.0 - 1.0
        return img

    def postprocess(self, preds, img, orig_img):
        preds = ops.non_max_suppression(preds,
                                        self.args.conf,
                                        self.args.iou,
                                        agnostic=self.args.agnostic_nms,
                                        max_det=self.args.max_det)

        for i, pred in enumerate(preds):
            # Person í´ë˜ìŠ¤ë§Œ í•„í„°ë§
            if pred is not None and len(pred):
                pred = pred[pred[:, 5] == 0]  # í´ë˜ìŠ¤ 0ì€ 'Person'ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
            shape = orig_img[i].shape if self.webcam else orig_img.shape
            if pred is not None and len(pred):
                pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], shape).round()
            preds[i] = pred

        return preds

    def write_results(self, idx, preds, batch):
        p, im, im0 = batch
        all_outputs = []
        log_string = ""
        if len(im.shape) == 3:
            im = im[None]  # expand for batch dim
        self.seen += 1
        im0 = im0.copy()
        if self.webcam:  # batch_size >= 1
            log_string += f'{idx}: '
            frame = self.dataset.count
        else:
            frame = getattr(self.dataset, 'frame', 0)

        self.data_path = p
        log_string += '%gx%g ' % im.shape[2:]  # print string
        self.annotator = self.get_annotator(im0)

        # ì²« ë²ˆì§¸ í”„ë ˆì„ì—ì„œ VideoWriter ì´ˆê¸°í™”
        if self.video_writer is None:
            # í”„ë ˆì„ í¬ê¸° ì„¤ì •
            self.frame_height, self.frame_width = im0.shape[:2]
            # VideoWriter ì´ˆê¸°í™”
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # ì½”ë± ì„¤ì • ('mp4v', 'XVID' ë“±)
            video_save_path = str(self.object_save_dir / 'output_video.mp4')  # ì €ì¥í•  ë¹„ë””ì˜¤ ê²½ë¡œ
            print(f"ë¹„ë””ì˜¤ ì €ì¥ ê²½ë¡œ: {video_save_path}")
            self.video_writer = cv2.VideoWriter(video_save_path, fourcc, self.fps, (self.frame_width, self.frame_height))

        det = preds[idx]
        all_outputs.append(det)
        if det is None or len(det) == 0:
            # í˜„ì¬ í”„ë ˆì„ì— ê°ì§€ëœ ê°ì²´ê°€ ì—†ì„ ê²½ìš°, ê¸°ì¡´ ê°ì²´ê°€ ì‚¬ë¼ì¡ŒëŠ”ì§€ ì²´í¬
            self.check_left_objects(frame)
            # í”„ë ˆì„ì„ ê·¸ëŒ€ë¡œ ì €ì¥
            if self.video_writer is not None:
                self.video_writer.write(im0)
            return log_string

        # í´ë˜ìŠ¤ ì¹´ìš´íŠ¸ ë¡œê·¸
        for c in det[:, 5].unique():
            n = (det[:, 5] == c).sum()  # detections per class
            log_string += f"{n} {self.model.names[int(c)]}{'s' * (n > 1)}, "

        xywh_bboxs = []
        confs = []
        oids = []
        outputs = []

        for *xyxy, conf, cls in reversed(det):
            x_c, y_c, bbox_w, bbox_h = xyxy_to_xywh(*xyxy)
            xywh_obj = [x_c, y_c, bbox_w, bbox_h]
            xywh_bboxs.append(xywh_obj)
            confs.append([conf.item()])
            oids.append(int(cls))
        xywhs = torch.Tensor(xywh_bboxs)
        confss = torch.Tensor(confs)

        outputs = deepsort.update(xywhs, confss, oids, im0)
        current_ids = []
        if len(outputs) > 0:
            bbox_xyxy = outputs[:, :4]
            identities = outputs[:, -2]
            object_id = outputs[:, -1]
            current_ids = identities.tolist()

            # ê° ê°ì²´ì˜ ì‹œê°„ ì—…ë°ì´íŠ¸
            for i, obj_id in enumerate(identities):
                obj_id = int(obj_id)
                if obj_id in self.tracked_objects:
                    obj = self.tracked_objects[obj_id]
                    if obj["last_seen"] == frame - 1:
                        obj["frames_seen"] += 1
                    else:
                        obj["frames_seen"] += 1  # í”„ë ˆì„ì´ ì—°ì†ë˜ì§€ ì•Šë”ë¼ë„ ëˆ„ì 
                    obj["last_seen"] = frame
                else:
                    # ìƒˆë¡œìš´ ê°ì²´ ì¶”ì  ì‹œì‘
                    self.tracked_objects[obj_id] = {"frames_seen": 1, "last_seen": frame}

                # ì‹œê°„ ê³„ì‚° ë° ì €ì¥
                time_on_screen = self.tracked_objects[obj_id]["frames_seen"] / self.fps
                self.object_times[obj_id] = time_on_screen

                # ë…¸ì¶œ ì‹œê°„ì´ 1ì´ˆë¥¼ ì´ˆê³¼í•˜ê³ , ì•„ì§ ì €ì¥ë˜ì§€ ì•Šì€ ê°ì²´ë¼ë©´ ì´ë¯¸ì§€ ì €ì¥
                if time_on_screen > 1.0 and obj_id not in self.saved_objects:
                    self.save_object_image(im0, bbox_xyxy[i], obj_id)
                    self.saved_objects.add(obj_id)

            # ê°ì²´ë³„ ì‹œê°„ ì •ë³´ë¥¼ draw_boxesì— ì „ë‹¬
            draw_boxes(im0, bbox_xyxy, self.model.names, object_id, identities, self.object_times)

        # í˜„ì¬ í”„ë ˆì„ì—ì„œ ì‚¬ë¼ì§„ ê°ì²´ ì²´í¬
        self.check_left_objects(frame, current_ids)

        # ì²˜ë¦¬ëœ í”„ë ˆì„ì„ ë¹„ë””ì˜¤ë¡œ ì €ì¥
        if self.video_writer is not None:
            self.video_writer.write(im0)

        # ê²°ê³¼ë¥¼ í‘œì‹œí•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì½”ë“œë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        if self.args.show:
            cv2.imshow(str(p), im0)
            cv2.waitKey(1)  # 1 millisecond

        return log_string

    def save_object_image(self, img, bbox, obj_id, min_size=80):
        """ê°ì²´ì˜ ì´ë¯¸ì§€ë¥¼ ë°”ìš´ë”© ë°•ìŠ¤ë¡œ ì˜ë¼ ì €ì¥í•©ë‹ˆë‹¤.
        Args:
            img: í˜„ì¬ í”„ë ˆì„ ì´ë¯¸ì§€
            bbox: ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ (x1, y1, x2, y2)
            obj_id: ê°ì²´ ID
            min_size: ê°ì²´ ì €ì¥ì„ ìœ„í•œ ìµœì†Œ í¬ê¸° (í”½ì…€)
        """
        x1, y1, x2, y2 = [int(coord) for coord in bbox]
        w, h = x2 - x1, y2 - y1

        # ë„ˆë¹„ì™€ ë†’ì´ë¥¼ ìµœì†Œ í¬ê¸°ë¡œ í™•ì¥
        if w < min_size:
            diff = (min_size - w) // 2
            x1 -= diff
            x2 += diff
        if h < min_size:
            diff = (min_size - h) // 2
            y1 -= diff
            y2 += diff

        # ì´ë¯¸ì§€ ê²½ê³„ë¥¼ ë„˜ì–´ê°€ì§€ ì•Šë„ë¡ ì œí•œ
        x1 = max(x1, 0)
        y1 = max(y1, 0)
        x2 = min(x2, img.shape[1])  # img.shape[1]ì€ ì´ë¯¸ì§€ì˜ ë„ˆë¹„
        y2 = min(y2, img.shape[0])  # img.shape[0]ì€ ì´ë¯¸ì§€ì˜ ë†’ì´

        # ìµœì¢… ë„ˆë¹„ì™€ ë†’ì´ë¥¼ ë‹¤ì‹œ ê³„ì‚°
        w, h = x2 - x1, y2 - y1

        # ë„ˆë¹„ì™€ ë†’ì´ê°€ ì—¬ì „íˆ ìµœì†Œ í¬ê¸°ë³´ë‹¤ ì‘ì€ ê²½ìš° ì¶”ê°€ í™•ì¥
        if w < min_size:
            x2 = min(x2 + (min_size - w), img.shape[1])
            x1 = max(x1 - (min_size - w), 0)
        if h < min_size:
            y2 = min(y2 + (min_size - h), img.shape[0])
            y1 = max(y1 - (min_size - h), 0)

        # ìµœì¢… ë°”ìš´ë”© ë°•ìŠ¤ í¬ê¸° í™•ì¸
        w, h = x2 - x1, y2 - y1
        assert w >= min_size and h >= min_size, "Bounding box resizing failed to meet minimum size"

        # ë°”ìš´ë”© ë°•ìŠ¤ í¬ê¸° ì¡°ì • í›„ ì´ë¯¸ì§€ ì˜ë¼ë‚´ê¸°
        obj_img = img[y1:y2, x1:x2]
        obj_img_path = self.object_save_dir / f"object_{obj_id}.jpg"
        cv2.imwrite(str(obj_img_path), obj_img)
        print(f"ID {obj_id} ê°ì²´ì˜ ì´ë¯¸ì§€ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤. í¬ê¸°: {obj_img.shape[1]}x{obj_img.shape[0]}")

    def check_left_objects(self, current_frame, current_ids=[]):
        # í˜„ì¬ ì¶”ì  ì¤‘ì¸ ê°ì²´ ì¤‘ì—ì„œ current_idsì— ì—†ëŠ” ê°ì²´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        for obj_id in list(self.tracked_objects.keys()):
            obj = self.tracked_objects[obj_id]
            if obj_id not in current_ids:
                frames_missing = current_frame - obj["last_seen"]
                time_missing = frames_missing / self.fps
                if time_missing >= 0.05:
                    # ê°ì²´ê°€ í™”ë©´ì—ì„œ ë²—ì–´ë‚¬ë‹¤ê³  íŒë‹¨
                    total_time = obj["frames_seen"] / self.fps
                    print(f"ID {obj_id} ê°ì²´ê°€ í™”ë©´ì—ì„œ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. ì´ ë…¸ì¶œ ì‹œê°„: {total_time:.2f}ì´ˆ")
                    # ê°ì²´ ì •ë³´ ì‚­ì œ
                    del self.tracked_objects[obj_id]
                    if obj_id in self.object_times:
                        del self.object_times[obj_id]
                    if obj_id in data_deque:
                        del data_deque[obj_id]
                    if obj_id in self.saved_objects:
                        self.saved_objects.remove(obj_id)

    def on_predict_end(self):
        # ëª¨ë“  ì˜ˆì¸¡ì´ ëë‚œ í›„ ë‚¨ì•„ìˆëŠ” ê°ì²´ì˜ ì´ ì‹œê°„ì„ ì¶œë ¥
        print("ë‚¨ì•„ìˆëŠ” ê°ì²´ì˜ í™”ë©´ ë…¸ì¶œ ì‹œê°„:")
        for obj_id, obj_info in self.tracked_objects.items():
            total_time = obj_info["frames_seen"] / self.fps
            print(f"ID {obj_id}: {total_time:.2f}ì´ˆ")

        # VideoWriter í•´ì œ
        if self.video_writer is not None:
            self.video_writer.release()
            print("ë¹„ë””ì˜¤ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

@hydra.main(version_base=None, config_path=str(DEFAULT_CONFIG.parent), config_name=DEFAULT_CONFIG.name)
def predict(cfg):
    torch.backends.cudnn.benchmark = True  # ì„±ëŠ¥ ìµœì í™”

    # ì‹¤í–‰ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()

    init_tracker()
    cfg.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    cfg.model = cfg.model or "yolov8n.pt"
    cfg.imgsz = check_imgsz(cfg.imgsz, min_dim=2)  # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
    cfg.source = cfg.source if cfg.source is not None else ROOT / "assets"
    predictor = DetectionPredictor(cfg)
    predictor()

    # ì‹¤í–‰ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
    end_time = time.time()
    elapsed_time = end_time - start_time  # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    print(f"ì „ì²´ ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")  # ì‹¤í–‰ ì‹œê°„ ì¶œë ¥

if __name__ == "__main__":
    predict()
